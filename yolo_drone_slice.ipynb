{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Object Detection on MP4 Videos\n",
    "\n",
    "This python code will do object detection on mp4 videos using the YOLO object detection vision model. The model will try to predict and put bounding boxes on each frame of the video. The output is a new mp4 video with bouding boxes embedded in the video.\n",
    "\n",
    "This implimentation will take the input video resolution and 'slice' it into smaller image squares (e.g., 640x640 pixels) to do the predictions. \n",
    "\n",
    "User inputs include the video path, the output path, the model to use, the classes to detect, and the confidence threshold.\n",
    "\n",
    "The model used is the YOLOv8 model that has been fine tuned on the [WALDO dataset](https://huggingface.co/StephanST/WALDO30). The dataset itself is not public, but the weights of this fine tuned model are available on Hugging Face. WALDO has been trained to identify 12 different objects. 0 = LightVehicle, 1 = Person, 2 = Building, 3 = UPole, 4 = Boat, 5 = Bike, 6 = Container, 7 = Truck, 8 = Gastank, 9 = Digger, 10 = SolarPanels, 11 = Bus. \n",
    "\n",
    "The WALDO fine tuned model is available on Hugging Face [here](https://huggingface.co/StephanST/WALDO30/resolve/main/WALDO30_yolov8m_640x640.pt?download=true).\n",
    "\n",
    "After running prediction on your videos, you can choose to fine-tune the model on your own dataset to improve results. The code for that is also available in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from sahi.auto_model import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.slicing import slice_coco\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using Apple Silicon, uncomment the following line to enable MPS (Metal Performance Shaders) for PyTorch\n",
    "\n",
    "import torch\n",
    "# Check if MPS is available\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##See what classes are in the Waldo fine-tuned model fetched from Hugging Face (https://huggingface.co/StephanST/WALDO30)\n",
    "\n",
    "model_waldo = YOLO(\"/Users/jgillan/Documents/repositories/yolo_drone/WALDO30_yolov8m_640x640.pt\")\n",
    "print(model_waldo.names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### User defined parameters #########\n",
    "input_video_path = '/Users/jgillan/Documents/repositories/yolo_drone/scr_drone_clip.mp4'\n",
    "output_video_path = '/Users/jgillan/Documents/repositories/yolo_drone/scr_drone_clip_predict.mp4'\n",
    "model_path = '/Users/jgillan/Documents/repositories/yolo_drone/WALDO30_yolov8m_640x640.pt'\n",
    "\n",
    "\n",
    "TARGET_CLASSES = [0] #eg, LightVehicle, Person, Building, etc\n",
    "confidence_threshold = 0.2\n",
    "\n",
    "slice_height = int(640)\n",
    "slice_width = int(640)\n",
    "overlap_height_ratio = float(0.1)\n",
    "overlap_width_ratio = float(0.1)\n",
    "\n",
    "# Create bounding box and label annotators\n",
    "#box_annotator = sv.BoundingBoxAnnotator(thickness=1)\n",
    "box_annotator = sv.BoxCornerAnnotator(thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Runs the prediction and outputs a new mp4 video \n",
    "\n",
    "# Initialize the YOLOv8 model\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence_threshold,\n",
    "    device='mps'  # or 'cpu' or \"cuda\" depending on your setup\n",
    ")\n",
    "\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "\n",
    "# Set up output video writer\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform sliced inference on the current frame using SAHI\n",
    "    \n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=detection_model,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio\n",
    "    )\n",
    "\n",
    "    # Extract data from SAHI result\n",
    "    object_predictions = [\n",
    "        pred for pred in result.object_prediction_list if pred.category.id in TARGET_CLASSES\n",
    "    ]    \n",
    "\n",
    "    # Initialize lists to hold the data\n",
    "    xyxy = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    class_names = []\n",
    "\n",
    "    # Loop over the object predictions and extract data\n",
    "    for pred in object_predictions:\n",
    "        bbox = pred.bbox.to_xyxy()  # Convert bbox to [x1, y1, x2, y2]\n",
    "        xyxy.append(bbox)\n",
    "        confidences.append(pred.score.value)\n",
    "        class_ids.append(pred.category.id)\n",
    "        class_names.append(pred.category.name)\n",
    "\n",
    "    # Check if there are any detections\n",
    "    if xyxy:\n",
    "        # Convert lists to numpy arrays\n",
    "        xyxy = np.array(xyxy, dtype=np.float32)\n",
    "        confidences = np.array(confidences, dtype=np.float32)\n",
    "        class_ids = np.array(class_ids, dtype=int)\n",
    "\n",
    "        # Create sv.Detections object\n",
    "        detections = sv.Detections(\n",
    "            xyxy=xyxy,\n",
    "            confidence=confidences,\n",
    "            class_id=class_ids\n",
    "        )\n",
    "\n",
    "        # Prepare labels for label annotator\n",
    "        labels = [\n",
    "            f\"{class_name} {confidence:.2f}\"\n",
    "            for class_name, confidence in zip(class_names, confidences)\n",
    "        ]\n",
    "\n",
    "        # Annotate frame with detection results\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "    else:\n",
    "        # If no detections, use the original frame\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Processed frame {frame_count}\", end='\\r')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"\\nInference complete. Video saved at\", output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This cell is similar to the previous, but uses a Tracker and also a smoother function\n",
    "##Runs the prediction and outputs a new mp4 video\n",
    "\n",
    "# Initialize the YOLOv8 model\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence_threshold,\n",
    "    device='mps'  # or 'cpu' or \"cuda\" depending on your setup\n",
    ")\n",
    "\n",
    "# Get video info\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=input_video_path)\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "# Set up output video writer\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize tracker and smoother\n",
    "tracker = sv.ByteTrack(frame_rate=video_info.fps)\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "# Create bounding box and label annotators\n",
    "box_annotator = sv.BoxCornerAnnotator(thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    text_scale=0.5,\n",
    "    text_thickness=1,\n",
    "    text_padding=1\n",
    ")\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "class_id_to_name = {}  # Initialize once to store class_id to name mapping\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform sliced inference on the current frame using SAHI\n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=detection_model,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio\n",
    "    )\n",
    "\n",
    "    # Extract data from SAHI result\n",
    "    object_predictions = result.object_prediction_list\n",
    "\n",
    "    # Initialize lists to hold the data\n",
    "    xyxy = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    # Build or update class_id to name mapping\n",
    "    for pred in object_predictions:\n",
    "        if pred.category.id not in class_id_to_name:\n",
    "            class_id_to_name[pred.category.id] = pred.category.name\n",
    "\n",
    "    # Loop over the object predictions and extract data\n",
    "    for pred in object_predictions:\n",
    "        bbox = pred.bbox.to_xyxy()  # Convert bbox to [x1, y1, x2, y2]\n",
    "        xyxy.append(bbox)\n",
    "        confidences.append(pred.score.value)\n",
    "        class_ids.append(pred.category.id)\n",
    "\n",
    "    # Check if there are any detections\n",
    "    if xyxy:\n",
    "        # Convert lists to numpy arrays\n",
    "        xyxy = np.array(xyxy, dtype=np.float32)\n",
    "        confidences = np.array(confidences, dtype=np.float32)\n",
    "        class_ids = np.array(class_ids, dtype=int)\n",
    "\n",
    "        # Create sv.Detections object\n",
    "        detections = sv.Detections(\n",
    "            xyxy=xyxy,\n",
    "            confidence=confidences,\n",
    "            class_id=class_ids\n",
    "        )\n",
    "\n",
    "        # Update tracker with detections\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "\n",
    "        # Update smoother with detections\n",
    "        detections = smoother.update_with_detections(detections)\n",
    "\n",
    "        # Prepare labels for label annotator\n",
    "        # Include tracker ID in labels if available\n",
    "        labels = []\n",
    "        for i in range(len(detections.xyxy)):\n",
    "            class_id = detections.class_id[i]\n",
    "            confidence = detections.confidence[i]\n",
    "            class_name = class_id_to_name.get(class_id, 'Unknown')\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "\n",
    "            # Add tracker ID if available\n",
    "            if hasattr(detections, 'tracker_id') and detections.tracker_id is not None:\n",
    "                tracker_id = detections.tracker_id[i]\n",
    "                label = f\"ID {tracker_id} {label}\"\n",
    "\n",
    "            labels.append(label)\n",
    "\n",
    "        # Annotate frame with detection results\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections\n",
    "        )\n",
    "        annotated_frame = label_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections,\n",
    "            labels=labels\n",
    "        )\n",
    "    else:\n",
    "        # If no detections, use the original frame\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Processed frame {frame_count}\", end='\\r')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"\\nInference complete. Video saved at\", output_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN    \n",
    "\n",
    "The following code will train the YOLOv8 model on your own labeled dataset. The dataset should be in the YOLOv8 format, which includes a .yaml file with the class names and a folder with images and labels. I recommend using Roboflow to label your images and export them in the YOLOv8 format. The labels (classes and order) should be exactly the same as the WALDO dataset.\n",
    "\n",
    "['LightVehicle', 'Person', 'Building', 'UPole', 'Boat', 'Bike', 'Container', 'Truck', 'Gastank', 'Digger', 'SolarPanels', 'Bus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This code block will re-map the class IDs output from Roboflow to match the IDs in the WALDO pretrained model \n",
    "\n",
    "roboflow_to_model = {\n",
    "    0: 2,  # Roboflow's 'Building' should be ID 2\n",
    "    1: 11,  # Roboflow's 'Bus'  should be ID 11\n",
    "    2: 0,  # Roboflow's 'LightVehicle' should be ID 0 \n",
    "    3: 1,\n",
    "    4: 10,\n",
    "    5: 7,\n",
    "}\n",
    "\n",
    "\n",
    "label_dir = \"/home/jgillan/Documents/yolo_drone/drone_detect.v4i.yolov8/train/labels\"\n",
    "\n",
    "for file in os.listdir(label_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        path = os.path.join(label_dir, file)\n",
    "        new_lines = []\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                old_class = int(parts[0])\n",
    "                new_class = roboflow_to_model.get(old_class)\n",
    "                if new_class is not None:\n",
    "                    parts[0] = str(new_class)\n",
    "                    new_lines.append(\" \".join(parts))\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(new_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User Inputs #######\n",
    "original_images_dir = Path(\"/home/jgillan/Documents/yolo_drone/drone_detect.v4i.yolov8/train/images\")\n",
    "original_labels_dir = Path(\"/home/jgillan/Documents/yolo_drone/drone_detect.v4i.yolov8/train/labels\")\n",
    "\n",
    "working_dir = Path(\"/home/jgillan/Documents/yolo_drone/drone_detect.v4i.yolov8/sliced\")  \n",
    "coco_json_path = working_dir / \"coco.json\"\n",
    "slice_output_name = working_dir / \"coco_sliced\"\n",
    "\n",
    "sliced_coco_json_path = f\"{slice_output_name}_coco.json\"\n",
    "sliced_images_dir = working_dir / \"images\"\n",
    "sliced_labels_dir = working_dir / \"labels\"\n",
    "\n",
    "class_names = ['LightVehicle', 'Person', 'Building', 'UPole', 'Boat', 'Bike', 'Container', 'Truck', 'Gastank', 'Digger', 'SolarPanels', 'Bus']  # Edit for your classes\n",
    "imgsz = 640\n",
    "overlap = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Slice the Video training data into Smaller tiles #######\n",
    "#It will write new training labels/images to the working_dir\n",
    "\n",
    "# ==== Clean output dirs ====\n",
    "if working_dir.exists():\n",
    "    shutil.rmtree(working_dir)\n",
    "sliced_images_dir.mkdir(parents=True)\n",
    "sliced_labels_dir.mkdir(parents=True)\n",
    "\n",
    "# ==== CONVERT YOLO → COCO ====\n",
    "def convert_yolo_to_coco(images_dir, labels_dir, class_name_list, output_json_path):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "\n",
    "    for image_file in sorted(Path(images_dir).glob(\"*\")):\n",
    "        if image_file.suffix.lower() not in ['.jpg', '.jpeg', '.png']:\n",
    "            continue\n",
    "        img = cv2.imread(str(image_file))\n",
    "        if img is None:\n",
    "            continue\n",
    "        height, width = img.shape[:2]\n",
    "        images.append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": image_file.name,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "        label_file = Path(labels_dir) / (image_file.stem + \".txt\")\n",
    "        if label_file.exists():\n",
    "            with open(label_file) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    class_id, x_center, y_center, w_rel, h_rel = map(float, parts)\n",
    "                    x = (x_center - w_rel / 2) * width\n",
    "                    y = (y_center - h_rel / 2) * height\n",
    "                    w = w_rel * width\n",
    "                    h = h_rel * height\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(class_id),\n",
    "                        \"bbox\": [x, y, w, h],\n",
    "                        \"area\": w * h,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "        img_id += 1\n",
    "\n",
    "    categories = [{\"id\": i, \"name\": name} for i, name in enumerate(class_name_list)]\n",
    "    coco_dict = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(coco_dict, f, indent=2)\n",
    "\n",
    "convert_yolo_to_coco(original_images_dir, original_labels_dir, class_names, coco_json_path)\n",
    "\n",
    "# ==== SLICE COCO DATASET ====\n",
    "slice_coco(\n",
    "    coco_annotation_file_path=str(coco_json_path),\n",
    "    image_dir=str(original_images_dir),\n",
    "    output_coco_annotation_file_name=str(slice_output_name),\n",
    "    ignore_negative_samples=True,\n",
    "    output_dir=str(sliced_images_dir),\n",
    "    slice_height=imgsz,\n",
    "    slice_width=imgsz,\n",
    "    overlap_height_ratio=overlap,\n",
    "    overlap_width_ratio=overlap,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ==== CONVERT SLICED COCO → YOLO ====\n",
    "def convert_coco_to_yolo(coco_json_path, output_label_dir, class_names):\n",
    "    with open(coco_json_path) as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    output_label_dir = Path(output_label_dir)\n",
    "    output_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build lookup dicts\n",
    "    image_lookup = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "    category_lookup = {cat[\"id\"]: cat[\"name\"] for cat in coco[\"categories\"]}\n",
    "\n",
    "    # Collect annotations per image\n",
    "    annotations_per_image = {}\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        image_id = ann[\"image_id\"]\n",
    "        if image_id not in annotations_per_image:\n",
    "            annotations_per_image[image_id] = []\n",
    "        annotations_per_image[image_id].append(ann)\n",
    "\n",
    "    for image_id, image_info in image_lookup.items():\n",
    "        file_name = Path(image_info[\"file_name\"])\n",
    "        width = image_info[\"width\"]\n",
    "        height = image_info[\"height\"]\n",
    "\n",
    "        yolo_lines = []\n",
    "\n",
    "        anns = annotations_per_image.get(image_id, [])\n",
    "        for ann in anns:\n",
    "            cat_id = ann[\"category_id\"]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to YOLO format\n",
    "            x_center = (x + w / 2) / width\n",
    "            y_center = (y + h / 2) / height\n",
    "            w_norm = w / width\n",
    "            h_norm = h / height\n",
    "            class_id = class_names.index(category_lookup[cat_id])\n",
    "            yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\")\n",
    "\n",
    "        if yolo_lines:\n",
    "            label_path = output_label_dir / (file_name.stem + \".txt\")\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "# Use the function\n",
    "convert_coco_to_yolo(\n",
    "    coco_json_path=sliced_coco_json_path,\n",
    "    output_label_dir=sliced_labels_dir,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ==== WRITE data.yaml ====\n",
    "data_yaml_path = working_dir / \"data.yaml\"\n",
    "with open(data_yaml_path, \"w\") as f:\n",
    "    f.write(f\"train: ../images\\n\")\n",
    "    f.write(f\"val: ../images\\n\")\n",
    "    f.write(f\"nc: {len(class_names)}\\n\")\n",
    "    f.write(f\"names: {class_names}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Run!!!! ########\n",
    "\n",
    "#model = YOLO(\"/home/jgillan/Documents/yolo_drone/WALDO30_yolov8m_640x640.pt\") # Path to the Waldo pre-trained model\n",
    "model = YOLO(\"/home/jgillan/Documents/repositories/yolo_drone/runs/detect/train4/weights/best.pt\") # path to my fine-tuned model\n",
    "results = model.train(data=str(data_yaml_path), lr0=0.01, epochs=20, freeze=22, patience=15, imgsz=imgsz, batch=8, device=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##View all the layers of the network\n",
    "\n",
    "model.model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training has completed, the fine-tuned model will be saved to '/home/jgillan/Documents/yolo_drone/runs/detect/train2/weights/best.pt'\n",
    "\n",
    "You can plug this path into 'model_path' in the prediction code above to predict on the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gillan_model = YOLO(\"/home/jgillan/Documents/repositories/yolo_drone/runs/detect/train2/weights/best.pt\")\n",
    "print(gillan_model.names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_drone_slice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
