{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Object Detection on MP4 Videos\n",
    "\n",
    "This python code will do object detection on mp4 videos using the YOLO object detection vision model. The model will try to predict and put bounding boxes on each frame of the video. The output is a new mp4 video with bouding boxes embedded in the video.\n",
    "\n",
    "This implimentation will take the input video resolution and 'slice' it into smaller image squares (e.g., 640x640 pixels) to do the predictions. \n",
    "\n",
    "User inputs include the video path, the output path, the model to use, the classes to detect, and the confidence threshold.\n",
    "\n",
    "The model used is the YOLOv8 model that has been fine tuned on the [WALDO dataset](https://huggingface.co/StephanST/WALDO30). The dataset itself is not public, but the weights of this fine tuned model are available on Hugging Face. WALDO has been trained to identify 12 different objects. 0 = light vehicle; 1 = person; 2 = building; 3 = Utility pole; 4 = boat; 5 = bike; 6 = container; 7 = truck; 8 = gastank; 10 = digger (construction equipment); 11 = solar panels; 12 = bus. \n",
    "\n",
    "The WALDO fine tuned model is available on Hugging Face [here](https://huggingface.co/StephanST/WALDO30/resolve/main/WALDO30_yolov8m_640x640.pt?download=true).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from sahi.auto_model import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import supervision as sv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User defined parameters\n",
    "input_video_path = '/home/jgillan/Documents/yolo_drone/2_italians.mp4'\n",
    "output_video_path = '/home/jgillan/Documents/yolo_drone/2_italians_predict4.mp4'\n",
    "model_path = '/home/jgillan/Documents/yolo_drone/WALDO30_yolov8m_640x640.pt'\n",
    "TARGET_CLASSES = [0, 1] #eg, for vehicle & person\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "slice_height = int(640)\n",
    "slice_width = int(640)\n",
    "overlap_height_ratio = float(0.1)\n",
    "overlap_width_ratio = float(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Runs the prediction and outputs a new mp4 video \n",
    "\n",
    "# Initialize the YOLOv8 model\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence_threshold,\n",
    "    device='cuda'  # or 'cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "\n",
    "# Set up output video writer\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "# Create bounding box and label annotators\n",
    "#box_annotator = sv.BoundingBoxAnnotator(thickness=1)\n",
    "box_annotator = sv.BoxCornerAnnotator(thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform sliced inference on the current frame using SAHI\n",
    "    \n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=detection_model,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio\n",
    "    )\n",
    "\n",
    "    # Extract data from SAHI result\n",
    "    object_predictions = [\n",
    "        pred for pred in result.object_prediction_list if pred.category.id in TARGET_CLASSES\n",
    "    ]    \n",
    "\n",
    "    # Initialize lists to hold the data\n",
    "    xyxy = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    class_names = []\n",
    "\n",
    "    # Loop over the object predictions and extract data\n",
    "    for pred in object_predictions:\n",
    "        bbox = pred.bbox.to_xyxy()  # Convert bbox to [x1, y1, x2, y2]\n",
    "        xyxy.append(bbox)\n",
    "        confidences.append(pred.score.value)\n",
    "        class_ids.append(pred.category.id)\n",
    "        class_names.append(pred.category.name)\n",
    "\n",
    "    # Check if there are any detections\n",
    "    if xyxy:\n",
    "        # Convert lists to numpy arrays\n",
    "        xyxy = np.array(xyxy, dtype=np.float32)\n",
    "        confidences = np.array(confidences, dtype=np.float32)\n",
    "        class_ids = np.array(class_ids, dtype=int)\n",
    "\n",
    "        # Create sv.Detections object\n",
    "        detections = sv.Detections(\n",
    "            xyxy=xyxy,\n",
    "            confidence=confidences,\n",
    "            class_id=class_ids\n",
    "        )\n",
    "\n",
    "        # Prepare labels for label annotator\n",
    "        labels = [\n",
    "            f\"{class_name} {confidence:.2f}\"\n",
    "            for class_name, confidence in zip(class_names, confidences)\n",
    "        ]\n",
    "\n",
    "        # Annotate frame with detection results\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "    else:\n",
    "        # If no detections, use the original frame\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Processed frame {frame_count}\", end='\\r')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"\\nInference complete. Video saved at\", output_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
