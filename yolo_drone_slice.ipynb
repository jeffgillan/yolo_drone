{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Object Detection on MP4 Videos\n",
    "\n",
    "This python code will do object detection on mp4 videos using the YOLO object detection vision model. The model will try to predict and put bounding boxes on each frame of the video. The output is a new mp4 video with bouding boxes embedded in the video.\n",
    "\n",
    "This implimentation will take the input video resolution and 'slice' it into smaller image squares (e.g., 640x640 pixels) to do the predictions. \n",
    "\n",
    "User inputs include the video path, the output path, the model to use, the classes to detect, and the confidence threshold.\n",
    "\n",
    "The model used is the YOLOv8 model that has been fine tuned on the [WALDO dataset](https://huggingface.co/StephanST/WALDO30). The dataset itself is not public, but the weights of this fine tuned model are available on Hugging Face. WALDO has been trained to identify 12 different objects. 0 = LightVehicle, 1 = Person, 2 = Building, 3 = UPole, 4 = Boat, 5 = Bike, 6 = Container, 7 = Truck, 8 = Gastank, 10 = Digger, 11 = SolarPanels, 12 = Bus. \n",
    "\n",
    "The WALDO fine tuned model is available on Hugging Face [here](https://huggingface.co/StephanST/WALDO30/resolve/main/WALDO30_yolov8m_640x640.pt?download=true).\n",
    "\n",
    "After running prediction on your videos, you can choose to fine-tune the model on your own dataset to improve results. The code for that is also available in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from sahi.auto_model import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### User defined parameters #########\n",
    "input_video_path = '/home/jgillan/Documents/yolo_drone/2_italians.mp4'\n",
    "output_video_path = '/home/jgillan/Documents/yolo_drone/2_italians_predict10.mp4'\n",
    "#model_path = '/home/jgillan/Documents/yolo_drone/WALDO30_yolov8m_640x640.pt'\n",
    "model_path = '/home/jgillan/Documents/yolo_drone/runs/detect/train2/weights/best.pt'\n",
    "\n",
    "\n",
    "TARGET_CLASSES = [0, 1] #eg, for vehicle & person\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "slice_height = int(640)\n",
    "slice_width = int(640)\n",
    "overlap_height_ratio = float(0.1)\n",
    "overlap_width_ratio = float(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Runs the prediction and outputs a new mp4 video \n",
    "\n",
    "# Initialize the YOLOv8 model\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=confidence_threshold,\n",
    "    device='cuda'  # or 'cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "\n",
    "# Set up output video writer\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "# Create bounding box and label annotators\n",
    "#box_annotator = sv.BoundingBoxAnnotator(thickness=1)\n",
    "box_annotator = sv.BoxCornerAnnotator(thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(text_scale=0.5, text_thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform sliced inference on the current frame using SAHI\n",
    "    \n",
    "    result = get_sliced_prediction(\n",
    "        image=frame,\n",
    "        detection_model=detection_model,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_height_ratio,\n",
    "        overlap_width_ratio=overlap_width_ratio\n",
    "    )\n",
    "\n",
    "    # Extract data from SAHI result\n",
    "    object_predictions = [\n",
    "        pred for pred in result.object_prediction_list if pred.category.id in TARGET_CLASSES\n",
    "    ]    \n",
    "\n",
    "    # Initialize lists to hold the data\n",
    "    xyxy = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    class_names = []\n",
    "\n",
    "    # Loop over the object predictions and extract data\n",
    "    for pred in object_predictions:\n",
    "        bbox = pred.bbox.to_xyxy()  # Convert bbox to [x1, y1, x2, y2]\n",
    "        xyxy.append(bbox)\n",
    "        confidences.append(pred.score.value)\n",
    "        class_ids.append(pred.category.id)\n",
    "        class_names.append(pred.category.name)\n",
    "\n",
    "    # Check if there are any detections\n",
    "    if xyxy:\n",
    "        # Convert lists to numpy arrays\n",
    "        xyxy = np.array(xyxy, dtype=np.float32)\n",
    "        confidences = np.array(confidences, dtype=np.float32)\n",
    "        class_ids = np.array(class_ids, dtype=int)\n",
    "\n",
    "        # Create sv.Detections object\n",
    "        detections = sv.Detections(\n",
    "            xyxy=xyxy,\n",
    "            confidence=confidences,\n",
    "            class_id=class_ids\n",
    "        )\n",
    "\n",
    "        # Prepare labels for label annotator\n",
    "        labels = [\n",
    "            f\"{class_name} {confidence:.2f}\"\n",
    "            for class_name, confidence in zip(class_names, confidences)\n",
    "        ]\n",
    "\n",
    "        # Annotate frame with detection results\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "    else:\n",
    "        # If no detections, use the original frame\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Processed frame {frame_count}\", end='\\r')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"\\nInference complete. Video saved at\", output_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN    \n",
    "\n",
    "The following code will train the YOLOv8 model on your own labeled dataset. The dataset should be in the YOLOv8 format, which includes a .yaml file with the class names and a folder with images and labels. I recommend using Roboflow to label your images and export them in the YOLOv8 format. The labels (classes and order) should be exactly the same as the WALDO dataset.\n",
    "\n",
    "['LightVehicle', 'Person', 'Building', 'UPole', 'Boat', 'Bike', 'Container', 'Truck', 'Gastank', 'Digger', 'SolarPanels', 'Bus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "from sahi.slicing import slice_coco\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User Inputs #######\n",
    "original_images_dir = Path(\"/home/jgillan/Documents/yolo_drone/drone_detect.v2i.yolov8/train/images\")\n",
    "original_labels_dir = Path(\"/home/jgillan/Documents/yolo_drone/drone_detect.v2i.yolov8/train/labels\")\n",
    "\n",
    "working_dir = Path(\"/home/jgillan/Documents/yolo_drone/drone_detect.v2i.yolov8/sliced\")  \n",
    "coco_json_path = working_dir / \"coco.json\"\n",
    "slice_output_name = working_dir / \"coco_sliced\"\n",
    "\n",
    "sliced_coco_json_path = f\"{slice_output_name}_coco.json\"\n",
    "sliced_images_dir = working_dir / \"images\"\n",
    "sliced_labels_dir = working_dir / \"labels\"\n",
    "\n",
    "class_names = ['LightVehicle', 'Person', 'Building', 'UPole', 'Boat', 'Bike', 'Container', 'Truck', 'Gastank', 'Digger', 'SolarPanels', 'Bus']  # Edit for your classes\n",
    "imgsz = 640\n",
    "overlap = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Slice the Video training data into Smaller tiles #######\n",
    "#It will write new training labels/images to the working_dir\n",
    "\n",
    "# ==== Clean output dirs ====\n",
    "if working_dir.exists():\n",
    "    shutil.rmtree(working_dir)\n",
    "sliced_images_dir.mkdir(parents=True)\n",
    "sliced_labels_dir.mkdir(parents=True)\n",
    "\n",
    "# ==== CONVERT YOLO → COCO ====\n",
    "def convert_yolo_to_coco(images_dir, labels_dir, class_name_list, output_json_path):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "\n",
    "    for image_file in sorted(Path(images_dir).glob(\"*\")):\n",
    "        if image_file.suffix.lower() not in ['.jpg', '.jpeg', '.png']:\n",
    "            continue\n",
    "        img = cv2.imread(str(image_file))\n",
    "        if img is None:\n",
    "            continue\n",
    "        height, width = img.shape[:2]\n",
    "        images.append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": image_file.name,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "        label_file = Path(labels_dir) / (image_file.stem + \".txt\")\n",
    "        if label_file.exists():\n",
    "            with open(label_file) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    class_id, x_center, y_center, w_rel, h_rel = map(float, parts)\n",
    "                    x = (x_center - w_rel / 2) * width\n",
    "                    y = (y_center - h_rel / 2) * height\n",
    "                    w = w_rel * width\n",
    "                    h = h_rel * height\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(class_id),\n",
    "                        \"bbox\": [x, y, w, h],\n",
    "                        \"area\": w * h,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "        img_id += 1\n",
    "\n",
    "    categories = [{\"id\": i, \"name\": name} for i, name in enumerate(class_name_list)]\n",
    "    coco_dict = {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(coco_dict, f, indent=2)\n",
    "\n",
    "convert_yolo_to_coco(original_images_dir, original_labels_dir, class_names, coco_json_path)\n",
    "\n",
    "# ==== SLICE COCO DATASET ====\n",
    "slice_coco(\n",
    "    coco_annotation_file_path=str(coco_json_path),\n",
    "    image_dir=str(original_images_dir),\n",
    "    output_coco_annotation_file_name=str(slice_output_name),\n",
    "    ignore_negative_samples=True,\n",
    "    output_dir=str(sliced_images_dir),\n",
    "    slice_height=imgsz,\n",
    "    slice_width=imgsz,\n",
    "    overlap_height_ratio=overlap,\n",
    "    overlap_width_ratio=overlap,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ==== CONVERT SLICED COCO → YOLO ====\n",
    "def convert_coco_to_yolo(coco_json_path, output_label_dir, class_names):\n",
    "    with open(coco_json_path) as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    output_label_dir = Path(output_label_dir)\n",
    "    output_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build lookup dicts\n",
    "    image_lookup = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "    category_lookup = {cat[\"id\"]: cat[\"name\"] for cat in coco[\"categories\"]}\n",
    "\n",
    "    # Collect annotations per image\n",
    "    annotations_per_image = {}\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        image_id = ann[\"image_id\"]\n",
    "        if image_id not in annotations_per_image:\n",
    "            annotations_per_image[image_id] = []\n",
    "        annotations_per_image[image_id].append(ann)\n",
    "\n",
    "    for image_id, image_info in image_lookup.items():\n",
    "        file_name = Path(image_info[\"file_name\"])\n",
    "        width = image_info[\"width\"]\n",
    "        height = image_info[\"height\"]\n",
    "\n",
    "        yolo_lines = []\n",
    "\n",
    "        anns = annotations_per_image.get(image_id, [])\n",
    "        for ann in anns:\n",
    "            cat_id = ann[\"category_id\"]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to YOLO format\n",
    "            x_center = (x + w / 2) / width\n",
    "            y_center = (y + h / 2) / height\n",
    "            w_norm = w / width\n",
    "            h_norm = h / height\n",
    "            class_id = class_names.index(category_lookup[cat_id])\n",
    "            yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\")\n",
    "\n",
    "        if yolo_lines:\n",
    "            label_path = output_label_dir / (file_name.stem + \".txt\")\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "# Use the function\n",
    "convert_coco_to_yolo(\n",
    "    coco_json_path=sliced_coco_json_path,\n",
    "    output_label_dir=sliced_labels_dir,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ==== WRITE data.yaml ====\n",
    "data_yaml_path = working_dir / \"data.yaml\"\n",
    "with open(data_yaml_path, \"w\") as f:\n",
    "    f.write(f\"train: ../images\\n\")\n",
    "    f.write(f\"val: ../images\\n\")\n",
    "    f.write(f\"nc: {len(class_names)}\\n\")\n",
    "    f.write(f\"names: {class_names}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##See what classes are in the Waldo fine-tuned model\n",
    "\n",
    "model_waldo = YOLO(\"/home/jgillan/Documents/yolo_drone/WALDO30_yolov8m_640x640.pt\")\n",
    "print(list(model_waldo.names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Run!!!! ########\n",
    "\n",
    "model = YOLO(\"/home/jgillan/Documents/yolo_drone/WALDO30_yolov8m_640x640.pt\") # Path to the Waldo pre-trained model\n",
    "results = model.train(data=str(data_yaml_path), epochs=10, imgsz=imgsz, batch=8, device=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training has completed, the fine-tuned model will be saved to '/home/jgillan/Documents/yolo_drone/runs/detect/train2/weights/best.pt'\n",
    "\n",
    "You can plug this path into 'model_path' in the prediction code above to predict on the video. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
